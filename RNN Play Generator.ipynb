{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read the contents of file\n",
    "\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8') # read and then decode for py2 compat\n",
    "\n",
    "print('Length of text: {} characters'.format(len(text))) # length of text is the number of character in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at thr first 250 character in text \n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "# create a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# look at how part of the text is encoded\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "# converting numeric value into text\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  # length of aequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)   # creating training examples/targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "Example\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(2):\n",
    "    print('\\n\\nExample\\n')\n",
    "    print('INPUT')\n",
    "    print(int_to_text(x))\n",
    "    print('\\nOUTPUT')\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab) # vocab is the number of unique chatacters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biulding the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                 batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) #(batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch) # model for prediction on first batch of training data\n",
    "    print(example_batch_predictions.shape, '#(batch_size, sequence_length, vocab_size)') # output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 2.6835676e-04 -2.4043592e-03  3.1238701e-04 ...  3.3250034e-03\n",
      "    1.4485762e-03 -1.8889762e-03]\n",
      "  [-9.5811859e-04 -6.5937079e-03  2.9485396e-03 ... -3.5521188e-03\n",
      "    1.3828911e-03 -3.6397122e-04]\n",
      "  [-6.4356113e-04 -4.7039390e-03  6.9149891e-03 ... -2.3314385e-03\n",
      "    1.0031664e-03 -1.2065803e-03]\n",
      "  ...\n",
      "  [-1.6279254e-02  9.0237241e-05  6.3311821e-03 ... -9.6180793e-03\n",
      "   -1.4927818e-03  3.5123213e-03]\n",
      "  [-9.8446803e-03  2.0734710e-04 -2.8090738e-04 ... -7.2063450e-03\n",
      "   -2.5813864e-03  2.3302324e-03]\n",
      "  [-7.4439961e-03  2.8094433e-03  7.7496096e-04 ... -1.0850264e-02\n",
      "   -2.3158535e-03 -4.9336534e-03]]\n",
      "\n",
      " [[ 3.2465232e-03 -8.5429556e-04  4.2326362e-03 ...  1.3357238e-03\n",
      "    5.5756699e-04  5.5873831e-04]\n",
      "  [ 2.7232147e-03 -6.6268737e-03  6.4409934e-03 ... -1.9845096e-03\n",
      "   -4.5434074e-03 -8.8995695e-03]\n",
      "  [ 1.1726246e-03 -1.7753309e-03  3.1910250e-03 ... -1.8854102e-03\n",
      "   -4.9156626e-03 -6.1342865e-03]\n",
      "  ...\n",
      "  [-1.3764877e-02  6.2270788e-03  5.8079173e-04 ... -5.3487555e-03\n",
      "   -2.9313485e-03  4.6264036e-03]\n",
      "  [-1.1166589e-02 -1.6504142e-03 -1.1697157e-03 ... -7.1349130e-03\n",
      "    3.0084597e-03 -1.0958328e-03]\n",
      "  [-1.6108716e-02 -2.6523019e-03 -4.6816736e-04 ... -7.2633298e-03\n",
      "    8.3075720e-06  2.6646997e-03]]\n",
      "\n",
      " [[ 2.6835676e-04 -2.4043592e-03  3.1238701e-04 ...  3.3250034e-03\n",
      "    1.4485762e-03 -1.8889762e-03]\n",
      "  [-3.9451034e-04  1.8255268e-03  6.2930188e-04 ...  6.8718968e-03\n",
      "   -3.9366065e-03  6.9689401e-04]\n",
      "  [-5.3587561e-03  1.6802781e-03  1.2292117e-03 ...  8.2336981e-03\n",
      "   -5.9267413e-03  2.7294026e-03]\n",
      "  ...\n",
      "  [-1.7107416e-02 -9.2584323e-03 -2.8188927e-03 ... -3.3178723e-03\n",
      "    1.8027601e-03  3.0871970e-03]\n",
      "  [-1.8230045e-02 -9.9632237e-03 -1.0627356e-03 ... -7.0079118e-03\n",
      "    6.0306946e-03 -2.9019152e-03]\n",
      "  [-1.9553857e-02 -2.6653295e-03 -1.0870991e-02 ... -1.6659103e-02\n",
      "    1.1465195e-02 -4.7719190e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-4.2165257e-03  6.6079320e-03 -1.1045815e-02 ... -1.2075803e-02\n",
      "    7.3490967e-03 -3.8706211e-03]\n",
      "  [-2.7709743e-03  9.6222777e-03 -1.1230912e-02 ... -1.0143748e-02\n",
      "    5.8875484e-03 -1.6306206e-03]\n",
      "  [ 2.1911236e-03  6.3708457e-03 -1.2891926e-02 ... -4.1577583e-03\n",
      "    2.2119414e-03 -2.5013089e-04]\n",
      "  ...\n",
      "  [-1.5483020e-02  1.0184551e-02 -5.6143608e-03 ... -1.5604338e-02\n",
      "   -2.3240801e-03  6.8657566e-03]\n",
      "  [-1.4978416e-02  5.6520207e-03 -2.9169756e-03 ... -1.9386476e-02\n",
      "    4.0192041e-05  2.2963285e-03]\n",
      "  [-2.2051429e-02  8.3530545e-03 -1.0772443e-02 ... -2.3365410e-02\n",
      "   -1.4250185e-03  1.3988734e-03]]\n",
      "\n",
      " [[-4.2165257e-03  6.6079320e-03 -1.1045815e-02 ... -1.2075803e-02\n",
      "    7.3490967e-03 -3.8706211e-03]\n",
      "  [-1.7147025e-03  5.5908780e-03 -6.1941398e-03 ... -1.1349115e-02\n",
      "    5.2167475e-03 -9.7091626e-03]\n",
      "  [ 4.3130154e-03  7.2023668e-03 -5.8830320e-03 ... -1.6030997e-02\n",
      "    3.1505770e-03 -8.5677765e-03]\n",
      "  ...\n",
      "  [-2.1073084e-02  2.4480314e-04 -9.1173854e-03 ... -1.9138174e-02\n",
      "    3.7750271e-03 -1.6883505e-04]\n",
      "  [-2.3095151e-02 -3.1595160e-03 -5.0626229e-03 ... -1.9328147e-02\n",
      "    4.1921204e-03 -5.6033721e-05]\n",
      "  [-2.1116396e-02 -5.3413743e-03 -2.6129279e-03 ... -2.0520043e-02\n",
      "    5.0231041e-03 -2.1087567e-03]]\n",
      "\n",
      " [[ 2.6835676e-04 -2.4043592e-03  3.1238701e-04 ...  3.3250034e-03\n",
      "    1.4485762e-03 -1.8889762e-03]\n",
      "  [-9.0404088e-04  6.2483083e-04 -2.1276199e-03 ...  1.6499958e-03\n",
      "   -2.8218483e-03 -6.9933257e-04]\n",
      "  [-2.9015669e-04  1.9916822e-03  8.4718055e-04 ... -1.1023553e-03\n",
      "   -3.3453405e-03 -7.6684467e-03]\n",
      "  ...\n",
      "  [-1.0170245e-02 -1.3717681e-03 -2.2917886e-03 ... -1.0385455e-03\n",
      "   -2.9724671e-03  1.0480284e-03]\n",
      "  [-9.0334322e-03 -5.9600114e-03 -4.4846348e-03 ... -2.5082920e-03\n",
      "    2.9715730e-03 -3.9430689e-03]\n",
      "  [-1.1863852e-02 -5.9379530e-03 -7.1491133e-03 ... -2.8589079e-03\n",
      "    5.4859179e-03 -1.2595155e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# prediction is an array of 64 arrays, one of each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 2.6835676e-04 -2.4043592e-03  3.1238701e-04 ...  3.3250034e-03\n",
      "   1.4485762e-03 -1.8889762e-03]\n",
      " [-9.5811859e-04 -6.5937079e-03  2.9485396e-03 ... -3.5521188e-03\n",
      "   1.3828911e-03 -3.6397122e-04]\n",
      " [-6.4356113e-04 -4.7039390e-03  6.9149891e-03 ... -2.3314385e-03\n",
      "   1.0031664e-03 -1.2065803e-03]\n",
      " ...\n",
      " [-1.6279254e-02  9.0237241e-05  6.3311821e-03 ... -9.6180793e-03\n",
      "  -1.4927818e-03  3.5123213e-03]\n",
      " [-9.8446803e-03  2.0734710e-04 -2.8090738e-04 ... -7.2063450e-03\n",
      "  -2.5813864e-03  2.3302324e-03]\n",
      " [-7.4439961e-03  2.8094433e-03  7.7496096e-04 ... -1.0850264e-02\n",
      "  -2.3158535e-03 -4.9336534e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 2.6835676e-04 -2.4043592e-03  3.1238701e-04  5.9055719e-03\n",
      "  2.7221891e-03  2.1151276e-03 -1.1146354e-03 -6.2866933e-03\n",
      "  3.0501164e-03 -9.3313772e-04 -3.1414132e-03  7.4931548e-04\n",
      " -6.9796210e-03  3.7527829e-03  1.6871274e-03 -2.6258470e-03\n",
      " -2.1284088e-03  5.6024562e-03  4.0740683e-03  3.9769299e-03\n",
      "  1.1814991e-03 -3.7159924e-03 -1.3651038e-03  2.3035039e-03\n",
      "  1.4176541e-03 -9.9157915e-05 -3.1870725e-03 -7.5181015e-04\n",
      "  5.3967666e-03 -5.1384382e-03 -1.5944138e-03  3.4170595e-03\n",
      "  2.2597297e-03  5.3067096e-03  5.7703359e-03 -3.1878176e-04\n",
      "  2.7990635e-03 -4.7373609e-03 -5.8932975e-04 -3.1357536e-03\n",
      "  4.8179915e-03  3.5158456e-03 -3.5370394e-04  1.3917428e-03\n",
      "  3.7823298e-03 -2.9317804e-03 -3.7185908e-03 -1.8853560e-04\n",
      "  3.2118117e-03 -9.8737655e-07 -5.0133364e-03 -5.0359429e-04\n",
      "  1.3695679e-03  2.6862170e-03  2.5242046e-03 -1.1091516e-04\n",
      "  3.1385622e-03  1.1578819e-04 -3.1051694e-03  1.5061484e-03\n",
      "  2.9909345e-03 -1.3696358e-03  3.3250034e-03  1.4485762e-03\n",
      " -1.8889762e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# prediction at the first timestamp\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "\n",
    "# 65 values representing the probability of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BysN wDF,du\\nU?zM3cPYMPI?mTCPGZ.ofNJYGrY,WM?!YkAFY&afK'!djkilunFddcGiaS,wkXqZkcM';bEYlOTBvNwuGRSoimV3\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to determine the predicted character we need to sample that putput edistribution\n",
    "\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "predicted_chars  # model prerdicted this for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "               filepath=checkpoint_prefix,\n",
    "               save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 437s 3s/step - loss: 2.5715\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 442s 3s/step - loss: 1.8779\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=2, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the model is finished training, we can find the latest checkpoint that stores the model weights\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    num_generate = 800 # number of character to generate\n",
    "    \n",
    "    input_eval = [char2idx[s] for s in start_string] # converting start string to numbers(vectorizing)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = [] # empty string to store result\n",
    "    \n",
    "    # low temperature results in more predictable text\n",
    "    # Higher temperature results in more surprising text\n",
    "    temperature= 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove batch dimension\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        \n",
    "        # using a categorical diatribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a starting string:p\n",
      "plouth,\n",
      "You deeder' theo, thing you this sobred'T': at I among:\n",
      "Of pruwe to you. in, whenchence atsey.\n",
      "All I am on hepperclant lotinus to balk\n",
      "And neach Pemesperte, with doving and 'cass of hesterps.\n",
      "\n",
      "IUSELDLUUS:\n",
      "Or cault not in.\n",
      "And thy son dow my loed, bettee's one cothen\n",
      "Edward\n",
      "Ey so, I beture but that cir these by anvorber,\n",
      "Nell hereir, det tor, And to dree thy butthal beenures,\n",
      "And it would ngive to marref of hows! if my will kishie,\n",
      "Wheerfore you and with it tive hig? Mind unde.\n",
      "\n",
      "BoPT:\n",
      "Manry, you lord, comen as that duts, it to but rears\n",
      "my lirsting tomet to forlem and malopracumell!\n",
      "Hat suringever mack the briviy to Unon-baltet,\n",
      "He carn to he book monedy so, sig,\n",
      "And good too yourt?\n",
      "\n",
      "LOYE:\n",
      "Noo, leave iffant Mithern youre ol kisters'd one\n",
      "Mare'cleam bile ann mery pasine\n",
      "Of sin, you bo\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string:\")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
